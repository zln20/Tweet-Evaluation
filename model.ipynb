{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset =load_dataset(\"tweet_eval\",\"emotion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_ckpt =\"distilbert-base-uncased\"\n",
    "tokenizer =AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\samya\\.cache\\huggingface\\datasets\\tweet_eval\\emotion\\1.1.0\\12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343\\cache-4e41ffc7235e5843.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1421 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\samya\\.cache\\huggingface\\datasets\\tweet_eval\\emotion\\1.1.0\\12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343\\cache-66bea20aaaf7f0e0.arrow\n"
     ]
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"],padding=True,truncation=True)\n",
    "tokenized_dataset=dataset.map(tokenize,batched=True,batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_projector', 'vocab_transform', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'pre_classifier', 'dropout_19']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "num_labels =4\n",
    "model=TFAutoModelForSequenceClassification.from_pretrained(model_ckpt,num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "from transformers import DataCollatorWithPadding\n",
    "data_collator =DataCollatorWithPadding(tokenizer=tokenizer,return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.set_format(type=\"torch\",columns=[\"input_ids\",\"text\",\"attention_mask\",\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Tensorflow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_dataset =tokenized_dataset[\"train\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\",\"attention_mask\"], \n",
    "    label_cols=[\"label\"], \n",
    "    shuffle=True, \n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "tf_valid_dataset=tokenized_dataset[\"validation\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\",\"attention_mask\"], \n",
    "    label_cols=[\"label\"], \n",
    "    shuffle=False, \n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=5e-5),\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=metrics.SparseCategoricalAccuracy()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "51/51 [==============================] - 439s 8s/step - loss: 0.9772 - sparse_categorical_accuracy: 0.5969 - val_loss: 0.6862 - val_sparse_categorical_accuracy: 0.7326\n",
      "Epoch 2/5\n",
      "51/51 [==============================] - 433s 9s/step - loss: 0.5150 - sparse_categorical_accuracy: 0.8262 - val_loss: 0.5969 - val_sparse_categorical_accuracy: 0.7968\n",
      "Epoch 3/5\n",
      "51/51 [==============================] - 436s 9s/step - loss: 0.2960 - sparse_categorical_accuracy: 0.9119 - val_loss: 0.6783 - val_sparse_categorical_accuracy: 0.7834\n",
      "Epoch 4/5\n",
      "51/51 [==============================] - 483s 9s/step - loss: 0.1729 - sparse_categorical_accuracy: 0.9493 - val_loss: 0.7322 - val_sparse_categorical_accuracy: 0.7914\n",
      "Epoch 5/5\n",
      "51/51 [==============================] - 510s 10s/step - loss: 0.0946 - sparse_categorical_accuracy: 0.9748 - val_loss: 0.8186 - val_sparse_categorical_accuracy: 0.7914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22c3f94c1c0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(tf_train_dataset,validation_data=tf_valid_dataset,epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thus, the validation accuracy of our model is 79%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a task related to NLP where I have used a dataset containing tweets and emotions. This is a text classification model in which I have used HuggingFace library to train the bert model. Firstly, I have preprocessed the dataset using AutoTokenizer function which converts unstructured text data into numerical array basd on mapping. Then, I have created train dataset for training our model and validation dataset for checking the accuracy of our model. After preprocessing, I have imported all the important libraries and have compiled the model using Adam optimizer, Sparse Categorical crossentropy for the loss function and sparse categorical accuracy for the metric. The final step of the model is training on the train dataset and checking the accuracy on the validation dataset. Our "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
